{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Yolov3 detector against kitti dataset\n",
    "In this notebook we aim to provide performance metrics for the YoloV3 detector by leveraging the kitti dataset videos and their ground truth labels. The procedure is as follows:  \n",
    "1) Load Yolov3 detector from the Einstein project.  \n",
    "2) Perform object detection on kitti video.  \n",
    "3) Evaluate accuracy of detections against kitti ground truth labels for given video.  \n",
    "#### Note:  \n",
    "We are using python from `/home/robert/PycharmProjects/Einstein/venv/bin/python`  \n",
    "## See public benchmark submissions here:\n",
    "http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=2d  \n",
    "## PASCAL criterion paper:\n",
    "http://host.robots.ox.ac.uk/pascal/VOC/voc2010/devkit_doc_08-May-2010.pdf  (See section 4.4)  \n",
    "## Official KITTI Object Detection Evaluation scripts:\n",
    "https://s3.eu-central-1.amazonaws.com/avg-kitti/devkit_object.zip  \n",
    "Also saved locally in this folder at `./devkit_object/`  \n",
    "See `./devkit_object/cpp/evaluate_object.cpp` for official evaluation code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load Yolov3 detector from the Einstein project: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/home/robert/caffe-0.15.9/python',\n",
       " '/usr/lib/python35.zip',\n",
       " '/usr/lib/python3.5',\n",
       " '/usr/lib/python3.5/plat-x86_64-linux-gnu',\n",
       " '/usr/lib/python3.5/lib-dynload',\n",
       " '/home/robert/PycharmProjects/Bob/venv/lib/python3.5/site-packages',\n",
       " '/home/robert/PycharmProjects/Bob/venv/lib/python3.5/site-packages/IPython/extensions',\n",
       " '/home/robert/.ipython',\n",
       " '/home/robert/PycharmProjects/Einstein/src']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/robert/PycharmProjects/Einstein/src')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error loading urlib2. Python2/3 mismatch?\n"
     ]
    }
   ],
   "source": [
    "from detection.car_detector_tf_v2 import CarDetectorTFV2\n",
    "from detection.car_detector_tf import CarDetectorTF\n",
    "detector = CarDetectorTFV2()\n",
    "#detector = CarDetectorTF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Perform object detection on kitti video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "\n",
    "import cv2\n",
    "import pykitti\n",
    "from PIL import Image\n",
    "\n",
    "date = '2011_09_26'\n",
    "#drive = '0001'\n",
    "drive = '0015'\n",
    "basedir = 'data'\n",
    "v2c_filepath = './data/2011_09_26/calib_velo_to_cam.txt'\n",
    "c2c_filepath = './data/2011_09_26/calib_cam_to_cam.txt'\n",
    "dataset = pykitti.raw(basedir, date, drive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing tracklet file data/2011_09_26/2011_09_26_drive_0015_sync/tracklet_labels.xml\n",
      "File contains 36 tracklets\n",
      "Loaded 36 tracklets.\n"
     ]
    }
   ],
   "source": [
    "from source import parseTrackletXML as xmlParser\n",
    "\n",
    "def load_tracklets_for_frames(n_frames, xml_path):\n",
    "    \"\"\"\n",
    "    Loads dataset labels also referred to as tracklets, saving them individually for each frame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_frames    : Number of frames in the dataset.\n",
    "    xml_path    : Path to the tracklets XML.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple of dictionaries with integer keys corresponding to absolute frame numbers and arrays as values. First array\n",
    "    contains coordinates of bounding box vertices for each object in the frame, and the second array contains objects\n",
    "    types as strings.\n",
    "    \"\"\"\n",
    "    tracklets = xmlParser.parseXML(xml_path)\n",
    "\n",
    "    frame_tracklets = {}\n",
    "    frame_tracklets_types = {}\n",
    "    for i in range(n_frames):\n",
    "        frame_tracklets[i] = []\n",
    "        frame_tracklets_types[i] = []\n",
    "\n",
    "    # loop over tracklets\n",
    "    for i, tracklet in enumerate(tracklets):\n",
    "        # this part is inspired by kitti object development kit matlab code: computeBox3D\n",
    "        h, w, l = tracklet.size\n",
    "        # in velodyne coordinates around zero point and without orientation yet\n",
    "        trackletBox = np.array([\n",
    "            [-l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2, l / 2, l / 2],\n",
    "            [w / 2, -w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2, w / 2],\n",
    "            [0.0, 0.0, 0.0, 0.0, h, h, h, h]\n",
    "        ])\n",
    "        # loop over all data in tracklet\n",
    "        for translation, rotation, state, occlusion, truncation, amtOcclusion, amtBorders, absoluteFrameNumber in tracklet:\n",
    "            # determine if object is in the image; otherwise continue\n",
    "            if truncation not in (xmlParser.TRUNC_IN_IMAGE, xmlParser.TRUNC_TRUNCATED):\n",
    "                continue\n",
    "            # re-create 3D bounding box in velodyne coordinate system\n",
    "            yaw = rotation[2]  # other rotations are supposedly 0\n",
    "            assert np.abs(rotation[:2]).sum() == 0, 'object rotations other than yaw given!'\n",
    "            rotMat = np.array([\n",
    "                [np.cos(yaw), -np.sin(yaw), 0.0],\n",
    "                [np.sin(yaw), np.cos(yaw), 0.0],\n",
    "                [0.0, 0.0, 1.0]\n",
    "            ])\n",
    "            cornerPosInVelo = np.dot(rotMat, trackletBox) + np.tile(translation, (8, 1)).T\n",
    "            frame_tracklets[absoluteFrameNumber] = frame_tracklets[absoluteFrameNumber] + [cornerPosInVelo]\n",
    "            frame_tracklets_types[absoluteFrameNumber] = frame_tracklets_types[absoluteFrameNumber] + [\n",
    "                tracklet.objectType]\n",
    "\n",
    "    return (frame_tracklets, frame_tracklets_types)\n",
    "tracklet_rects, tracklet_types = load_tracklets_for_frames(len(list(dataset.velo)), 'data/{}/{}_drive_{}_sync/tracklet_labels.xml'.format(date, date, drive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through cam2 files and perform detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, copy all necessary KITTI helper functions from eda notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     18,
     39,
     109,
     117,
     122,
     127,
     142,
     146
    ]
   },
   "outputs": [],
   "source": [
    "def calib_velo2cam(filepath):\n",
    "    \"\"\" \n",
    "    get Rotation(R : 3x3), Translation(T : 3x1) matrix info \n",
    "    using R,T matrix, we can convert velodyne coordinates to camera coordinates\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as f:\n",
    "        file = f.readlines()    \n",
    "        \n",
    "        for line in file:\n",
    "            (key, val) = line.split(':',1)\n",
    "            if key == 'R':\n",
    "                R = np.fromstring(val, sep=' ')\n",
    "                R = R.reshape(3, 3)\n",
    "            if key == 'T':\n",
    "                T = np.fromstring(val, sep=' ')\n",
    "                T = T.reshape(3, 1)\n",
    "    return R, T\n",
    "\n",
    "def calib_cam2cam(filepath, mode):\n",
    "    \"\"\"\n",
    "    If your image is 'rectified image' :\n",
    "        get only Projection(P : 3x4) matrix is enough\n",
    "    but if your image is 'distorted image'(not rectified image) :\n",
    "        you need undistortion step using distortion coefficients(5 : D)\n",
    "        \n",
    "    in this code, I'll get P matrix since I'm using rectified image\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as f:\n",
    "        file = f.readlines()\n",
    "        \n",
    "        for line in file:\n",
    "            (key, val) = line.split(':',1)\n",
    "            if key == ('P_rect_' + mode):\n",
    "                P_ = np.fromstring(val, sep=' ')\n",
    "                P_ = P_.reshape(3, 4)\n",
    "                # erase 4th column ([0,0,0])\n",
    "                P_ = P_[:3, :3]\n",
    "    return P_\n",
    " \n",
    "def velo3d_2_camera2d_points(points, v_fov, h_fov, vc_path, cc_path, mode='02'):\n",
    "    \"\"\" print velodyne 3D points corresponding to camera 2D image \"\"\"\n",
    "    # R_vc = Rotation matrix ( velodyne -> camera )\n",
    "    # T_vc = Translation matrix ( velodyne -> camera )\n",
    "    R_vc, T_vc = calib_velo2cam(vc_path)\n",
    "    \n",
    "    # P_ = Projection matrix ( camera coordinates 3d points -> image plane 2d points )\n",
    "    P_ = calib_cam2cam(cc_path, mode)\n",
    "\n",
    "    \"\"\"\n",
    "    xyz_v - 3D velodyne points corresponding to h, v FOV in the velodyne coordinates\n",
    "    c_    - color value(HSV's Hue) corresponding to distance(m)\n",
    "    \n",
    "             [x_1 , x_2 , .. ]\n",
    "    xyz_v =  [y_1 , y_2 , .. ]   \n",
    "             [z_1 , z_2 , .. ]\n",
    "             [ 1  ,  1  , .. ]\n",
    "    \"\"\"  \n",
    "    xyz_v, c_ = velo_points_filter(points, v_fov, h_fov)\n",
    "    \n",
    "    \"\"\"\n",
    "    RT_ - rotation matrix & translation matrix\n",
    "        ( velodyne coordinates -> camera coordinates )\n",
    "    \n",
    "            [r_11 , r_12 , r_13 , t_x ]\n",
    "    RT_  =  [r_21 , r_22 , r_23 , t_y ]   \n",
    "            [r_31 , r_32 , r_33 , t_z ]\n",
    "    \"\"\"\n",
    "    RT_ = np.concatenate((R_vc, T_vc),axis = 1)\n",
    "    \n",
    "    # convert velodyne coordinates(X_v, Y_v, Z_v) to camera coordinates(X_c, Y_c, Z_c) \n",
    "    for i in range(xyz_v.shape[1]):\n",
    "        xyz_v[:3,i] = np.matmul(RT_, xyz_v[:,i])\n",
    "        \n",
    "    \"\"\"\n",
    "    xyz_c - 3D velodyne points corresponding to h, v FOV in the camera coordinates\n",
    "             [x_1 , x_2 , .. ]\n",
    "    xyz_c =  [y_1 , y_2 , .. ]   \n",
    "             [z_1 , z_2 , .. ]\n",
    "    \"\"\" \n",
    "    xyz_c = np.delete(xyz_v, 3, axis=0)\n",
    "\n",
    "    # convert camera coordinates(X_c, Y_c, Z_c) image(pixel) coordinates(x,y) \n",
    "    for i in range(xyz_c.shape[1]):\n",
    "        xyz_c[:,i] = np.matmul(P_, xyz_c[:,i])    \n",
    "\n",
    "    \"\"\"\n",
    "    xy_i - 3D velodyne points corresponding to h, v FOV in the image(pixel) coordinates before scale adjustment\n",
    "    ans  - 3D velodyne points corresponding to h, v FOV in the image(pixel) coordinates\n",
    "             [s_1*x_1 , s_2*x_2 , .. ]\n",
    "    xy_i =   [s_1*y_1 , s_2*y_2 , .. ]        ans =   [x_1 , x_2 , .. ]  \n",
    "             [  s_1   ,   s_2   , .. ]                [y_1 , y_2 , .. ]\n",
    "    \"\"\"\n",
    "    xy_i = xyz_c[::]/xyz_c[::][2]\n",
    "    ans = np.delete(xy_i, 2, axis=0)\n",
    "    \n",
    "    \"\"\"\n",
    "    width = 1242\n",
    "    height = 375\n",
    "    w_range = in_range_points(ans[0], width)\n",
    "    h_range = in_range_points(ans[1], height)\n",
    "\n",
    "    ans_x = ans[0][np.logical_and(w_range,h_range)][:,None].T\n",
    "    ans_y = ans[1][np.logical_and(w_range,h_range)][:,None].T\n",
    "    c_ = c_[np.logical_and(w_range,h_range)]\n",
    "\n",
    "    ans = np.vstack((ans_x, ans_y))\n",
    "    \"\"\"\n",
    "    \n",
    "    return ans, c_\n",
    "def depth_color(val, min_d=0, max_d=120):\n",
    "    \"\"\" \n",
    "    print Color(HSV's H value) corresponding to distance(m) \n",
    "    close distance = red , far distance = blue\n",
    "    \"\"\"\n",
    "    np.clip(val, 0, max_d, out=val) # max distance is 120m but usually not usual\n",
    "    return (((val - min_d) / (max_d - min_d)) * 120).astype(np.uint8) \n",
    "\n",
    "def in_h_range_points(points, m, n, fov):\n",
    "    \"\"\" extract horizontal in-range points \"\"\"\n",
    "    return np.logical_and(np.arctan2(n,m) > (-fov[1] * np.pi / 180), \\\n",
    "                          np.arctan2(n,m) < (-fov[0] * np.pi / 180))\n",
    "\n",
    "def in_v_range_points(points, m, n, fov):\n",
    "    \"\"\" extract vertical in-range points \"\"\"\n",
    "    return np.logical_and(np.arctan2(n,m) < (fov[1] * np.pi / 180), \\\n",
    "                          np.arctan2(n,m) > (fov[0] * np.pi / 180))\n",
    "\n",
    "def fov_setting(points, x, y, z, dist, h_fov, v_fov):\n",
    "    \"\"\" filter points based on h,v FOV  \"\"\"\n",
    "    \n",
    "    if h_fov[1] == 180 and h_fov[0] == -180 and v_fov[1] == 2.0 and v_fov[0] == -24.9:\n",
    "        return points\n",
    "    \n",
    "    if h_fov[1] == 180 and h_fov[0] == -180:\n",
    "        return points[in_v_range_points(points, dist, z, v_fov)]\n",
    "    elif v_fov[1] == 2.0 and v_fov[0] == -24.9:        \n",
    "        return points[in_h_range_points(points, x, y, h_fov)]\n",
    "    else:\n",
    "        h_points = in_h_range_points(points, x, y, h_fov)\n",
    "        v_points = in_v_range_points(points, dist, z, v_fov)\n",
    "        return points[np.logical_and(h_points, v_points)]\n",
    "\n",
    "def in_range_points(points, size):\n",
    "    \"\"\" extract in-range points \"\"\"\n",
    "    return np.logical_and(points > 0, points < size)    \n",
    "\n",
    "def velo_points_filter(points, v_fov, h_fov):\n",
    "    \"\"\" extract points corresponding to FOV setting \"\"\"\n",
    "    \n",
    "    # Projecting to 2D\n",
    "    x = points[:, 0]\n",
    "    y = points[:, 1]\n",
    "    z = points[:, 2]\n",
    "    dist = np.sqrt(x ** 2 + y ** 2 + z ** 2)\n",
    "\n",
    "    if h_fov[0] < -90:\n",
    "        h_fov = (-90,) + h_fov[1:]\n",
    "    if h_fov[1] > 90:\n",
    "        h_fov = h_fov[:1] + (90,)\n",
    "    \n",
    "    x_lim = fov_setting(x, x, y, z, dist, h_fov, v_fov)[:,None]\n",
    "    y_lim = fov_setting(y, x, y, z, dist, h_fov, v_fov)[:,None]\n",
    "    z_lim = fov_setting(z, x, y, z, dist, h_fov, v_fov)[:,None]\n",
    "\n",
    "    # Stack arrays in sequence horizontally\n",
    "    xyz_ = np.hstack((x_lim, y_lim, z_lim))\n",
    "    xyz_ = xyz_.T\n",
    "\n",
    "    # stack (1,n) arrays filled with the number 1\n",
    "    one_mat = np.full((1, xyz_.shape[1]), 1)\n",
    "    xyz_ = np.concatenate((xyz_, one_mat),axis = 0)\n",
    "\n",
    "    # need dist info for points color\n",
    "    dist_lim = fov_setting(dist, x, y, z, dist, h_fov, v_fov)\n",
    "    color = depth_color(dist_lim, 0, 70)\n",
    "    \n",
    "    return xyz_, color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define some helper functions of our own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     19
    ]
   },
   "outputs": [],
   "source": [
    "def draw_detections(detections: list, img: np.ndarray, confidence_threshold:float, color=(0,255,0)):\n",
    "    \"\"\"\n",
    "    Draw detections to an image\n",
    "    Args:\n",
    "        detections: list of bounding boxes [[x1, y1, x2, y2], ..., ...]\n",
    "        img: image to draw detections to\n",
    "        confidence_threshold: (0.0 to 1.0) only draw detections above this threshold\n",
    "        color: (Optional) provide a color to draw the bounding boxes\n",
    "        \n",
    "    \"\"\"\n",
    "    bboxes, class_names, confidences = detections\n",
    "    for bbox, class_name, confidence in zip(bboxes, class_names, confidences):\n",
    "        #if class_name.lower() in ['car', 'truck', 'bus', 'van'] and confidence > confidence_threshold:\n",
    "        if True: # NOTE: just draw everything\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness=2)\n",
    "            text = '{} {:.1f}%'.format(class_name,\n",
    "                                           confidence * 100)\n",
    "            cv2.putText(img, text, (x1, y1-5), 1, 1.5, color, 2)\n",
    "\n",
    "def tracklets_to_bbox_labels(frame_tracklets, frame_labels):\n",
    "    \"\"\"\n",
    "    Convert 3d tracklets to 2D bounding boxes with corresponding labels\n",
    "    Args:\n",
    "        frame_tracklets: list of trackles for given frame\n",
    "        frame_labels: list of labels for given frame\n",
    "    Returns:\n",
    "        tuple: (bboxes, labels, confidences) for given frame\n",
    "    \"\"\"\n",
    "    bboxes, labels, confidences = [], [], []\n",
    "    for single_tracklet, single_label in zip(frame_tracklets, frame_labels):\n",
    "        single_tracklet_px, c_ = velo3d_2_camera2d_points(single_tracklet.T, v_fov=(-24.9, 2.0), h_fov=(-45,45), \\\n",
    "                                       vc_path=v2c_filepath, cc_path=c2c_filepath, mode='02')\n",
    "        single_tracklet_px = single_tracklet_px.astype(int).T\n",
    "        prev_pnt = single_tracklet_px[0]\n",
    "        x1 = single_tracklet_px[:, 0].min()\n",
    "        y1 = single_tracklet_px[:, 1].min()\n",
    "        x2 = single_tracklet_px[:, 0].max()\n",
    "        y2 = single_tracklet_px[:, 1].max()\n",
    "        \n",
    "        x_min_world = single_tracklet.T[:, 0].min()\n",
    "        y_avg_world = single_tracklet.T[:, 1].mean()\n",
    "        world_dist = np.sqrt(x_min_world**2 + y_avg_world**2)\n",
    "        bboxes.append([x1, y1, x2, y2])\n",
    "        labels.append(single_label)\n",
    "        confidences.append(1.0)\n",
    "    return bboxes, labels, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952dd06147484c63a84fc695b8433f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=297), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "all_detections, all_ground_truths = [], []\n",
    "PAUSE = False\n",
    "for frame_number in tqdm_notebook(range(len(dataset.cam2_files))):\n",
    "    img = np.array(dataset.get_cam2(frame_number))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    detections = detector.detect(img=img, return_class_scores=True)\n",
    "    all_detections.append(detections)\n",
    "    draw_detections(detections=detections, img=img, confidence_threshold=0.8, color=(0, 0, 255))\n",
    "    frame_tracklets = tracklet_rects[frame_number]\n",
    "    frame_labels = tracklet_types[frame_number]\n",
    "    detections_ground_truth = tracklets_to_bbox_labels(frame_tracklets, frame_labels)\n",
    "    all_ground_truths.append(detections_ground_truth)\n",
    "    draw_detections(detections=detections_ground_truth, img=img, confidence_threshold=0, color=(0,255,0))\n",
    "    cv2.imshow('image', img)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key & 0xFF == 27 or key == ord('q'):\n",
    "        break\n",
    "    elif key == ord('p'):\n",
    "        PAUSE = not PAUSE\n",
    "    while PAUSE is True:\n",
    "        cv2.imshow('image', img)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('p'):\n",
    "            PAUSE = not PAUSE\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Evaluate accuracy of detections against kitti ground truth labels for given video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection over union (IOU) between two bounding boxes\n",
    "Use this blog as reference:https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/  \n",
    "```\n",
    "Intersection over Union is an evaluation metric used to measure the accuracy of an object detector on a particular dataset. We often see this evaluation metric used in object detection challenges such as the popular PASCAL VOC challenge.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_intersection_over_union(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    " \n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    " \n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    " \n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    " \n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick example of calculating IOU for predicted vs. Ground Truth\n",
    "NOTE: ground truth and predicted coords taken directly from video by mouse cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED BBOX HEIGHT = 48\n",
      "GT BBOX HEIGHT = 53\n",
      "IOU = 0.6908752327746741\n"
     ]
    }
   ],
   "source": [
    "box_pred = [573, 176, 640, 224]\n",
    "box_gt = [588, 175, 643, 228]\n",
    "\n",
    "example_iou = bb_intersection_over_union(box_pred, box_gt)\n",
    "print(\"PRED BBOX HEIGHT = {}\".format(box_pred[3] - box_pred[1]))\n",
    "print(\"GT BBOX HEIGHT = {}\".format(box_gt[3] - box_gt[1]))\n",
    "\n",
    "print(\"IOU = {}\".format(example_iou))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First check one frame worth of detected bounding boxes versus ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections_1 = all_detections[1][0]\n",
    "ground_truth_1 = all_ground_truths[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[361, 178, 384, 192], [279, 181, 312, 199], [576, 175, 602, 204], [555, 164, 567, 174]]\n",
      "[[246, 173, 300, 202], [320, 175, 356, 196], [577, 176, 613, 212]]\n"
     ]
    }
   ],
   "source": [
    "print(detections_1)\n",
    "print(ground_truth_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[361, 178, 384, 192]\n",
      "[246, 173, 300, 202]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "det = detections_1[0]\n",
    "gt = ground_truth_1[0]\n",
    "print(det)\n",
    "print(gt)\n",
    "print(bb_intersection_over_union(det,gt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extend to all detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the following:  \n",
    "1) True Positives: E.g. a car is detected and overlaps with ground truth > MIN_IOU_THRESHOLD  \n",
    "2) False Positives:  E.g. a car is detected and does not overlap with any ground truth at all (overlap = 0 compared to all ground truths)  \n",
    "3) False Negatives: E.g. a car is not detected but there exists a ground truth that has no detection assigned to it.    \n",
    "These metrics will be used to generate Accuracy, Precision, and Recall  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of true positives: 161\n",
      "number of false positives: 396\n",
      "number of false negatives: 598\n",
      "PRECISION = 0.289048473967684\n",
      "RECALL = 0.21212121212121213\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "true_positives, false_positives, false_negatives = [], [], []\n",
    "MIN_IOU_THRESHOLD = 0.70  # as per KITTI standards\n",
    "MIN_FALSE_NEGATIVE_IOU = 0.0 # min threshold for determining whether false negative\n",
    "MIN_FALSE_POSITIVE_IOU = 0.0 # min threshold for determining whether false positive\n",
    "for detections, ground_truths in zip(all_detections, all_ground_truths):\n",
    "    detections = detections[0]   # bboxes at index 0 (assume labels are correct, for now)\n",
    "    ground_truths = ground_truths[0]  \n",
    "    pass_fail = []\n",
    "    # calculate true positive and false negatives\n",
    "    for gt in ground_truths:\n",
    "        false_negative = True\n",
    "        greatest_iou = 0.0\n",
    "        for det in detections:\n",
    "            iou = bb_intersection_over_union(gt, det)\n",
    "            if iou > greatest_iou:\n",
    "                greatest_iou = iou\n",
    "            if iou >= MIN_IOU_THRESHOLD:\n",
    "                true_positives.append([gt, det])\n",
    "            if iou > MIN_FALSE_NEGATIVE_IOU:\n",
    "                false_negative = False\n",
    "        if false_negative is True:\n",
    "            false_negatives.append([gt, det])\n",
    "    # calculate false positives\n",
    "    for det in detections:\n",
    "        false_positive = True\n",
    "        for gt in ground_truths:\n",
    "            iou = bb_intersection_over_union(gt, det)\n",
    "            if iou > MIN_FALSE_POSITIVE_IOU:\n",
    "                false_positive = False\n",
    "        if false_positive is True:\n",
    "            false_positives.append([det, ground_truths])\n",
    "\n",
    "precision = len(true_positives) / (len(true_positives) + len(false_positives))\n",
    "recall = len(true_positives) / (len(true_positives) + len(false_negatives))\n",
    "print(\"number of true positives: {}\".format(len(true_positives)))\n",
    "print(\"number of false positives: {}\".format(len(false_positives)))\n",
    "print(\"number of false negatives: {}\".format(len(false_negatives)))\n",
    "print(\"PRECISION = {}\".format(precision))\n",
    "print(\"RECALL = {}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick comparison of our v1 detector vs. v2 detector\n",
    "#### V1 detector (mobilenet):   \n",
    "number of true positives: 161  \n",
    "number of false positives: 396  \n",
    "number of false negatives: 598  \n",
    "PRECISION = 0.289048473967684  \n",
    "RECALL = 0.21212121212121213  \n",
    "#### V2 detector (YoloV3):  \n",
    "number of true positives: 379  \n",
    "number of false positives: 39  \n",
    "number of false negatives: 400  \n",
    "PRECISION = 0.9066985645933014  \n",
    "RECALL = 0.4865211810012837  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
